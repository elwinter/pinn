# Import standard modules.
from importlib import import_module
import os
import sys

# Import supplemental modules.
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Import project modules.
import pinn.standard_plots as psp

#-----------------------------------------------------------------------------

# Import the problem definition.

# Specify the run ID (aka problem name).
runid = "eplasma3"

# Add the subdirectory for the run results to the module search path.
run_path = os.path.join(".", runid)
sys.path.append(run_path)

# Import the problem definition from the run results directory.
p = import_module(runid)

# Read the run hyperparameters from the run results directory.
import hyperparameters as hp

#-----------------------------------------------------------------------------

# Load the data generated by the run.

# Load the training point coordinates.
# The format of each line is:
# t x
# where x increases from xmin to xmax fastest
# then t increases from tmin to tmax slowest
# Example:
# 0.0 0.0
# 0.0 0.1
# ...
# 0.0 0.9
# 0.0 1.0
# 1.0 0.0
# ...
# <HACK>
# n_train = nt*nx, where nt = nx (square grid)
# </HACK>
# Shape is (n_train, p.n_dim)
X_train = np.loadtxt(os.path.join(runid, "X_train.dat"))
# Shape is (n_train,)
t_train = X_train[:, p.it]  # All t-values
x_train = X_train[:, p.ix]  # All x-values

# <HACK>
# Compute the total number of training points, and the number of points
# along each dimension.
n_train = len(X_train)
nt = int(np.round(np.sqrt(n_train)))
nx = nt
# </HACK>

# Load the data locations and values (includes initial and boundary
# conditions).
# The format of each line is:
# t x n1 u1x E1x
# Shape is (n_data, p.n_dim + p.n_var)
XY_data = np.loadtxt(os.path.join(runid, "XY_data.dat"))

# Load the model-predicted values of variables and derivatives.
# Ytrain is a list of p.n_var np.ndarray() objects.
# Each array is shape (n_train,).
# delYtrain is a list of p.n_var np.ndarray() objects.
# Each array is shape (n_train, p.n_dim).
Ytrain = []
delYtrain = []
for var_name in p.dependent_variable_names:
    # Predicted variable values
    filename = f"{var_name}_train.dat"
    path = os.path.join(runid, filename)
    data = np.loadtxt(path)
    Ytrain.append(data)
    # Predicted gradient values
    filename = f"del_{var_name}_train.dat"
    path = os.path.join(runid, filename)
    data = np.loadtxt(path)
    delYtrain.append(data)

# Load the composite loss function histories for the individual models.
losses_model = np.loadtxt(os.path.join(runid, "losses_model.dat"))

# Load the equation residual loss function histories for the individual
# models.
losses_model_res = np.loadtxt(os.path.join(runid, "losses_model_res.dat"))

# Load the data loss function histories for the individual models.
losses_model_data = np.loadtxt(os.path.join(runid, "losses_model_data.dat"))

# Load the composite loss function history for the aggregated models.
losses = np.loadtxt(os.path.join(runid, "losses.dat"))

# Load the equation residual loss function history for the aggregated
# models.
losses_res = np.loadtxt(os.path.join(runid, "losses_res.dat"))

# Load the data loss function history for the aggregated models.
losses_data = np.loadtxt(os.path.join(runid, "losses_data.dat"))

#-----------------------------------------------------------------------------

# Compute analytical values for the variables and their gradients, and the
# errors in the predicted values.

# Yact is a list of p.n_var np.ndarray() objects.
# Each array is shape (n_train,).
# Yerr is the also shape (n_train,).
Yact = []
Yerr = []
for i in range(p.n_var):
    # Predicted variable values
    data = p.analytical_solutions[i](X_train)
    Yact.append(data)
    err = Ytrain[i] - Yact[i]
    Yerr.append(err)

#-----------------------------------------------------------------------------

# Compute the limits of the training domain.
t_min = t_train[0]
t_max = t_train[-1]
x_min = x_train[0]
x_max = x_train[-1]

# Extract the unique training point values (a grid is assumed).
t_train_vals = np.unique(t_train)
x_train_vals = np.unique(x_train)
n_t_train_vals = len(t_train_vals)
n_x_train_vals = len(x_train_vals)

#-----------------------------------------------------------------------------

# Plotting options

# Specify the heatmap value range for each variable.
vmin = [
    -1.0e-3, -1.0e-5, -1.0e-5, -1.0e-1, -1.0e-5, -1.0e-5, -1.0e-1, -1.0e-5, 
]
vmax = [
    1.0e-3, 1.0e-5, 1.0e-5, 1.0e-1, 1.0e-5, 1.0e-5, 1.0e-1, 1.0e-5, 
]
err_vmin = [
    -1.0e-3, -1.0e-5, -1.0e-5, -2.0e-1, -1.0e-5, -1.0e-5, -2.0e-1, -1.0e-5, 
]
err_vmax = [
    1.0e-3, 1.0e-5, 1.0e-5, 2.0e-1, 1.0e-5, 1.0e-5, 2.0e-1, 1.0e-5, 
]

# Specify the size (width, height) (in inches) for individual subplots.
SUBPLOT_WIDTH = 5.0
SUBPLOT_HEIGHT = 5.0

# Compute the coordinate plot tick locations and labels.
# XY_N_X_TICKS = 5
# x_min = x_train[0]
# XY_x_tick_pos = np.linspace(x_min, x_max, XY_N_X_TICKS)
# XY_x_tick_labels = ["%.1f" % x for x in XY_x_tick_pos]

# Compute the heat map tick locations and labels.
HEATMAP_N_X_TICKS = 5
heatmap_x_tick_pos = np.linspace(0, n_x_train_vals - 1, HEATMAP_N_X_TICKS)
heatmap_x_tick_labels = [
    "%.1f" % (x_min + x/(n_x_train_vals - 1)*(x_max - x_min))
    for x in heatmap_x_tick_pos
]
HEATMAP_N_Y_TICKS = 5
heatmap_y_tick_pos = np.linspace(0, n_t_train_vals - 1, HEATMAP_N_Y_TICKS)
heatmap_y_tick_labels = [
    "%.1f" % (t_min + t/(n_t_train_vals - 1)*(t_max - t_min))
    for t in heatmap_y_tick_pos
]
heatmap_y_tick_labels = list(reversed(heatmap_y_tick_labels))

#-----------------------------------------------------------------------------

# Create figures in a memory buffer.
mpl.use("Agg")

#-----------------------------------------------------------------------------

# Plot loss functions.

# Plot the loss history for each individual model.
# The result is a grid of plots of shape (p.n_var/2, 2).
# Each plot shows the residual, data, and composite loss for a single model.
fig = psp.plot_model_loss_functions(
    losses_model_res, losses_model_data, losses_model,
    p.dependent_variable_labels
)
plt.savefig("model_losses.png")

# Plot the total loss function history.
# The plot shows the residual, data, and composite loss for the aggregated
# model.
total_loss_figsize = (SUBPLOT_WIDTH*2, SUBPLOT_HEIGHT)
plt.figure(figsize=total_loss_figsize)
psp.plot_loss_functions(
    [losses_res, losses_data, losses],
    ["$L_{res}$", "$L_{data}$", "$L$"],
    title="Total loss function history for %s" % runid
)
plt.savefig("loss.png")

#-----------------------------------------------------------------------------

# Plot the analytical, predicted, and error values for each variable as heat
# maps.
for (i, var_name) in enumerate(p.dependent_variable_names):
    print(i, var_name)

    # Reshape the array so that the origin is in the lower-left corner of
    # the heatmap.
    actual = np.flip(Yact[i].reshape(nx, nt).T)
    predicted = np.flip(Ytrain[i].reshape(nx, nt).T)
    error = np.flip(Yerr[i].reshape(nx, nt).T)
    print(predicted.min(), predicted.max())
    print(error.min(), error.max())

    # Make a heat map of each set of values.
    # Horizontal axis: x
    # Vertical axis: t
    # Origin: Lower-left corner
    fig = psp.plot_actual_predicted_error(
        None, None, actual, predicted, error,
        vmin=vmin[i], vmax=vmax[i],
        err_vmin=err_vmin[i], err_vmax=err_vmax[i],
        title=p.dependent_variable_labels[i],
        x_tick_pos=heatmap_x_tick_pos, x_tick_labels=heatmap_x_tick_labels,
        y_tick_pos=heatmap_y_tick_pos, y_tick_labels=heatmap_y_tick_labels,
    )

    # Save the plot.
    filename = f"{var_name}.png"
    plt.savefig(filename)
